{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "access_token = '210555813636350|rKIeB30Wq0TWA-a0XoIC8OKBrpo'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "https://www.facebook.com/ads/library/?active_status=all&ad_type=all&country=US&impression_search_field=has_impressions_lifetime&view_all_page_id=153080620724"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ad_archive_id(data):\n",
    "    \"\"\"\n",
    "    Extract ad_archive_id from ad_snapshot_url\n",
    "    \"\"\"\n",
    "    return re.search(r\"/\\?id=([0-9]+)\", data[\"ad_snapshot_url\"]).group(1)\n",
    "\n",
    "\n",
    "class FbAdsLibraryTraversal:\n",
    "    default_url_pattern = (\n",
    "        \"https://graph.facebook.com/{}/ads_archive?access_token={}&\"\n",
    "        + \"fields={}&search_terms={}&ad_reached_countries={}&search_page_ids={}&\"\n",
    "        + \"ad_active_status={}&limit={}\"\n",
    "    )\n",
    "    default_api_version = \"v4.0\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        access_token,\n",
    "        fields,\n",
    "        search_term,\n",
    "        country,\n",
    "        search_page_ids=\"\",\n",
    "        ad_active_status=\"active\",\n",
    "        after_date=\"1970-01-01\",\n",
    "        page_limit=500,\n",
    "        api_version=None,\n",
    "        retry_limit=3,\n",
    "    ):\n",
    "        self.page_count = 0\n",
    "        self.access_token = access_token\n",
    "        self.fields = fields\n",
    "        self.search_term = search_term\n",
    "        self.country = country\n",
    "        self.after_date = after_date\n",
    "        self.search_page_ids = search_page_ids\n",
    "        self.ad_active_status = ad_active_status\n",
    "        self.page_limit = page_limit\n",
    "        self.retry_limit = retry_limit\n",
    "        if api_version is None:\n",
    "            self.api_version = self.default_api_version\n",
    "        else:\n",
    "            self.api_version = api_version\n",
    "\n",
    "    def generate_ad_archives(self):\n",
    "        next_page_url = self.default_url_pattern.format(\n",
    "            self.api_version,\n",
    "            self.access_token,\n",
    "            self.fields,\n",
    "            self.search_term,\n",
    "            self.country,\n",
    "            self.search_page_ids,\n",
    "            self.ad_active_status,\n",
    "            self.page_limit,\n",
    "        )\n",
    "        return self.__class__._get_ad_archives_from_url(\n",
    "            next_page_url, after_date=self.after_date, retry_limit=self.retry_limit\n",
    "        )\n",
    "\n",
    "    @staticmethod\n",
    "    def _get_ad_archives_from_url(\n",
    "        next_page_url, after_date=\"1970-01-01\", retry_limit=3\n",
    "    ):\n",
    "        last_error_url = None\n",
    "        last_retry_count = 0\n",
    "        start_time_cutoff_after = datetime.strptime(\n",
    "            after_date, \"%Y-%m-%d\"\n",
    "        ).timestamp()\n",
    "\n",
    "        while next_page_url is not None:\n",
    "            response = requests.get(next_page_url)\n",
    "            response_data = json.loads(response.text)\n",
    "            if \"error\" in response_data:\n",
    "                if next_page_url == last_error_url:\n",
    "                    # failed again\n",
    "                    if last_retry_count >= retry_limit:\n",
    "                        raise Exception(\n",
    "                            \"Error message: [{}], failed on URL: [{}]\".format(\n",
    "                                json.dumps(response_data[\"error\"]), next_page_url\n",
    "                            )\n",
    "                        )\n",
    "                else:\n",
    "                    last_error_url = next_page_url\n",
    "                    last_retry_count = 0\n",
    "                last_retry_count += 1\n",
    "                continue\n",
    "\n",
    "            filtered = list(\n",
    "                filter(\n",
    "                    lambda ad_archive: datetime.strptime(\n",
    "                        ad_archive[\"ad_delivery_start_time\"], \"%Y-%m-%dT%H:%M:%S%z\"\n",
    "                    ).timestamp()\n",
    "                    >= start_time_cutoff_after,\n",
    "                    response_data[\"data\"],\n",
    "                )\n",
    "            )\n",
    "            if len(filtered) == 0:\n",
    "                # if no data after the after_date, break\n",
    "                next_page_url = None\n",
    "                break\n",
    "            yield filtered\n",
    "\n",
    "            if \"paging\" in response_data:\n",
    "                next_page_url = response_data[\"paging\"][\"next\"]\n",
    "            else:\n",
    "                next_page_url = None\n",
    "\n",
    "    @classmethod\n",
    "    def generate_ad_archives_from_url(cls, failure_url, after_date=\"1970-01-01\"):\n",
    "        \"\"\"\n",
    "        if we failed from error, later we can just continue from the last failure url\n",
    "        \"\"\"\n",
    "        return cls._get_ad_archives_from_url(failure_url, after_date=after_date)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "fb_ads = FbAdsLibraryTraversal(access_token, 'all','Trump', 'United States')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = fb_ads.generate_ad_archives()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "send() takes exactly one argument (0 given)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-19-7c488c0efdda>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mgenerator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m: send() takes exactly one argument (0 given)"
     ]
    }
   ],
   "source": [
    "generator.send()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_ads(generator_ad_archives,is_verbose=False):\n",
    "    \"\"\"\n",
    "    Count how many ad_archives match your query\n",
    "    \"\"\"\n",
    "    count = 0\n",
    "    for ad_archives in generator_ad_archives:\n",
    "        count += len(ad_archives)\n",
    "        if is_verbose:\n",
    "            print(\"counting %d\" % count)\n",
    "    print(\"Total number of ads match the query: {}\".format(count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "[Errno 22] Invalid argument",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-13-616f2d9b042a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mcount_ads\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgenerator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-12-acb48e03cd5c>\u001b[0m in \u001b[0;36mcount_ads\u001b[1;34m(generator_ad_archives, is_verbose)\u001b[0m\n\u001b[0;32m      4\u001b[0m     \"\"\"\n\u001b[0;32m      5\u001b[0m     \u001b[0mcount\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m     \u001b[1;32mfor\u001b[0m \u001b[0mad_archives\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mgenerator_ad_archives\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m         \u001b[0mcount\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mad_archives\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mis_verbose\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-4-48db3e5c0021>\u001b[0m in \u001b[0;36m_get_ad_archives_from_url\u001b[1;34m(next_page_url, after_date, retry_limit)\u001b[0m\n\u001b[0;32m     64\u001b[0m         \u001b[0mlast_retry_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     65\u001b[0m         start_time_cutoff_after = datetime.strptime(\n\u001b[1;32m---> 66\u001b[1;33m             \u001b[0mafter_date\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"%Y-%m-%d\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     67\u001b[0m         ).timestamp()\n\u001b[0;32m     68\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mOSError\u001b[0m: [Errno 22] Invalid argument"
     ]
    }
   ],
   "source": [
    "count_ads(generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "import requests\n",
    "import csv\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "from itertools import product\n",
    "\n",
    "with open('config.yaml', 'r') as f:\n",
    "    config = yaml.safe_load(f)\n",
    "\n",
    "assert config['search_total'] % config['page_total'] == 0, \\\n",
    "    \"search_total should be a multiple of page_total.\"\n",
    "\n",
    "params = {\n",
    "    'access_token': config['access_token'],\n",
    "    'ad_type': 'POLITICAL_AND_ISSUE_ADS',\n",
    "    'ad_reached_countries': \"['US']\",\n",
    "    'ad_active_status': config['ad_active_status'],\n",
    "    'search_terms': config.get('search_terms'),\n",
    "    'search_page_ids': \",\".join(config.get('search_page_ids', [])),\n",
    "    'fields': \",\".join(config['query_fields']),\n",
    "    'limit': config['page_total']\n",
    "}\n",
    "\n",
    "REGIONS = set(config['regions'])\n",
    "DEMOS = set(product(config['demo_ages'], config['demo_genders']))\n",
    "\n",
    "f1 = open('fb_ads.csv', 'w')\n",
    "w1 = csv.DictWriter(f1, fieldnames=config['output_fields'],\n",
    "                    extrasaction='ignore')\n",
    "w1.writeheader()\n",
    "\n",
    "f2 = open('fb_ads_demos.csv', 'w')\n",
    "w2 = csv.DictWriter(f2, fieldnames=config['demo_fields'],\n",
    "                    extrasaction='ignore')\n",
    "w2.writeheader()\n",
    "\n",
    "f3 = open('fb_ads_regions.csv', 'w')\n",
    "w3 = csv.DictWriter(f3, fieldnames=config['region_fields'],\n",
    "                    extrasaction='ignore')\n",
    "w3.writeheader()\n",
    "\n",
    "pbar = tqdm(total=config['search_total'], smoothing=0)\n",
    "\n",
    "for _ in range(int(config['search_total'] / config['page_total'])):\n",
    "    r = requests.get('https://graph.facebook.com/v5.0/ads_archive',\n",
    "                     params=params)\n",
    "    data = r.json()\n",
    "    for ad in data['data']:\n",
    "        # The ad_id is encoded in the ad snapshot URL\n",
    "        # and cannot be accessed as a normal field. (?!?!)\n",
    "\n",
    "        ad_id = re.search(r'\\d+', ad['ad_snapshot_url']).group(0)\n",
    "        ad_url = 'https://www.facebook.com/ads/library/?id=' + ad_id\n",
    "\n",
    "        # write to the unnested files\n",
    "        demo_set = set()\n",
    "        for demo in ad['demographic_distribution']:\n",
    "            demo.update({'ad_id': ad_id})\n",
    "            w2.writerow(demo)\n",
    "            demo_set.add((demo['age'], demo['gender']))\n",
    "\n",
    "        # Impute a percentage of 0\n",
    "        # for demos with insufficient data\n",
    "        unused_demos = DEMOS - demo_set\n",
    "        for demo in unused_demos:\n",
    "            w2.writerow({\n",
    "                'ad_id': ad_id,\n",
    "                'age': demo[0],\n",
    "                'gender': demo[1],\n",
    "                'percentage': 0\n",
    "            })\n",
    "\n",
    "        region_set = set()\n",
    "        for region in ad['region_distribution']:\n",
    "            region.update({'ad_id': ad_id})\n",
    "            w3.writerow(region)\n",
    "            region_set.add(region['region'])\n",
    "\n",
    "        # Impute a percentage of 0\n",
    "        # for states with insufficient data\n",
    "        unused_regions = REGIONS - region_set\n",
    "        for region in unused_regions:\n",
    "            w3.writerow({\n",
    "                'ad_id': ad_id,\n",
    "                'region': region,\n",
    "                'percentage': 0\n",
    "            })\n",
    "\n",
    "        ad.update({'ad_id': ad_id,\n",
    "                   'ad_url': ad_url,\n",
    "                   'impressions_min': ad['impressions']['lower_bound'],\n",
    "                   'impressions_max': ad['impressions']['upper_bound'],\n",
    "                   'spend_min': ad['spend']['lower_bound'],\n",
    "                   'spend_max': ad['spend']['upper_bound'],\n",
    "                   })\n",
    "\n",
    "        w1.writerow(ad)\n",
    "        pbar.update()\n",
    "\n",
    "    # if we have scraped all the ads, exit\n",
    "    if 'paging' not in data:\n",
    "        break\n",
    "\n",
    "    params.update({'after': data['paging']['cursors']['after']})\n",
    "\n",
    "f1.close()\n",
    "f2.close()\n",
    "f3.close()\n",
    "pbar.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
